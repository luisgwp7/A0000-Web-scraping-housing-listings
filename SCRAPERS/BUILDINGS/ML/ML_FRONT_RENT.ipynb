{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries and modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import os \n",
    "import datetime\n",
    "from ML.frontpage import frontpage as fp, regexdata as rd\n",
    "from ML.directories import master_directory as md \n",
    "from ML.generate_url import returnUrlHousingtoScrap\n",
    "\n",
    "from ML.tools import savedata as sd\n",
    "#Visual de data (permite ver todo un df)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __ENVIRONMENT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables \n",
    "\n",
    "mth = 'FEB'       #Update each month\n",
    "yr = '2024'       #Update each month\n",
    "\n",
    "op = 'Rent'\n",
    "web = 'ML'\n",
    "\n",
    "#Master directory\n",
    "master_dir = md()\n",
    "master_dir = f'{master_dir}/{web}'\n",
    "\n",
    "\n",
    "#Directories to export files \n",
    "result_dir = f'{master_dir}/FRONTPAGE/JOIN'\n",
    "monitoring_dir = f'{master_dir}/FRONTPAGE/MONITORING/{yr}/{mth}'\n",
    "\n",
    "#Export files \n",
    "###############################################################################################################\n",
    "#This files is going to be joined for create the master file, which contains all the data for the fron page\n",
    "fl_pre = f'{web}_join_{op}_{mth}{yr}_'\n",
    "\n",
    "fl_master = f'{web}_{mth}{yr}_{op}_Front'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate links "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usted generó 330 enlaces principales de portada para arriendo\n"
     ]
    }
   ],
   "source": [
    "df_enlaces = returnUrlHousingtoScrap(op)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Se crearon 2 DataFrames para df_enlaces:\n",
      "ML_join_Rental_FEB2024_0\n",
      "ML_join_Rental_FEB2024_24\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "num_dfs = len(df_enlaces) // 24 + (1 if len(df_enlaces) % 24 != 0 else 0)\n",
    "# Dividir df_enlaces entre 5000 observaciones cada uno para hacer el scrapign sin problemas\n",
    "dfs = {f'{fl_pre}{i}': df_enlaces.iloc[i:i+24] for i in range(0, len(df_enlaces), 24)}\n",
    "# Nombres de df_enlaces\n",
    "print(f'Se crearon {num_dfs} DataFrames para df_enlaces:')\n",
    "for key in dfs:\n",
    "    print(f'{key}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# __Scraping__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El proceso de scraping ha comenzado a las 16:07 del 08/02/2024\n",
      "Realizando scraping en ML_join_Rental_FEB2024_0\n",
      "Enlace 1 de 24\n",
      "Enlace 2 de 24\n",
      "Enlace 3 de 24\n",
      "Enlace 4 de 24\n",
      "Enlace 5 de 24\n",
      "Enlace 6 de 24\n",
      "Enlace 7 de 24\n",
      "Enlace 8 de 24\n",
      "Enlace 9 de 24\n",
      "Enlace 10 de 24\n",
      "Enlace 11 de 24\n",
      "Enlace 12 de 24\n",
      "Enlace 13 de 24\n",
      "Enlace 14 de 24\n",
      "Enlace 15 de 24\n",
      "Enlace 16 de 24\n",
      "Enlace 17 de 24\n",
      "Enlace 18 de 24\n",
      "Enlace 19 de 24\n",
      "Enlace 20 de 24\n",
      "Enlace 21 de 24\n",
      "Enlace 22 de 24\n",
      "Enlace 23 de 24\n",
      "Enlace 24 de 24\n",
      "Scraping completado para ML_join_Rental_FEB2024_0.\n",
      "--------------------------------------------------\n",
      "Realizando scraping en ML_join_Rental_FEB2024_24\n",
      "Enlace 1 de 24\n",
      "Enlace 2 de 24\n",
      "Enlace 3 de 24\n",
      "Enlace 4 de 24\n",
      "Enlace 5 de 24\n",
      "Enlace 6 de 24\n",
      "Enlace 7 de 24\n",
      "Enlace 8 de 24\n",
      "Enlace 9 de 24\n",
      "Enlace 10 de 24\n",
      "Enlace 11 de 24\n",
      "Enlace 12 de 24\n",
      "Enlace 13 de 24\n",
      "Enlace 14 de 24\n",
      "Enlace 15 de 24\n",
      "Enlace 16 de 24\n",
      "Enlace 17 de 24\n",
      "Enlace 18 de 24\n",
      "Enlace 19 de 24\n",
      "Enlace 20 de 24\n",
      "Enlace 21 de 24\n",
      "Enlace 22 de 24\n",
      "Enlace 23 de 24\n",
      "Enlace 24 de 24\n",
      "Scraping completado para ML_join_Rental_FEB2024_24.\n",
      "--------------------------------------------------\n",
      "El proceso de scraping ha terminado a las 17:41 del 08/02/2024\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    start_time = datetime.datetime.now()\n",
    "    print(f\"El proceso de scraping ha comenzado a las {start_time.strftime('%H:%M')} del {start_time.strftime('%d/%m/%Y')}\")\n",
    "    \n",
    "    for df_key, df_value in dfs.items():\n",
    "        print(f\"Realizando scraping en {df_key}\")\n",
    "        global resultado_df\n",
    "        # Inicializa un DataFrame vacío para almacenar los resultados\n",
    "        resultado_df = pd.DataFrame()\n",
    "        contador_actual = 0\n",
    "        # Inicializa una lista para almacenar los resultados de cada enlace\n",
    "        resultados = []\n",
    "        # Itera a través de la lista de enlaces\n",
    "        for index, row in df_value.iterrows():\n",
    "            # Inicializar contador de enlaces procesados en la iteración actual\n",
    "            contador_actual += 1\n",
    "            print(f\"Enlace {contador_actual} de {len(df_value)}\")\n",
    "            \n",
    "            link = row[\"Enlace\"]\n",
    "            df_resultado = fp(link)  # Cambia el valor de 'pages' según tus necesidades\n",
    "            resultados.append(df_resultado)\n",
    "\n",
    "        # Concatena los resultados en un solo DataFrame\n",
    "        resultado_df = pd.concat(resultados, ignore_index=True)\n",
    "        \n",
    "        # sd -> save_to_csv()\n",
    "        sd(resultado_df, f'{result_dir}/{df_key}')\n",
    "\n",
    "        print(f\"Scraping completado para {df_key}.\")\n",
    "        print(\"--------------------------------------------------\")\n",
    "    #Fin del scraping: \n",
    "    end_time = datetime.datetime.now()\n",
    "    print(f\"El proceso de scraping ha terminado a las {end_time.strftime('%H:%M')} del {end_time.strftime('%d/%m/%Y')}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Se produjo un error en el scraping: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\luisG\\GIT\\SCRAPING\\SCRAPERS\\BUILDINGS\\ML\\ML\\frontpage.py:641: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
      "  df['Precio'] = df['Precio'].str.replace(\".\", \"\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6554\n"
     ]
    }
   ],
   "source": [
    "# Obtén la lista de nombres de archivo CSV en la carpeta\n",
    "archivos_csv = [archivo for archivo in os.listdir(result_dir) if archivo.endswith('.csv') and archivo.startswith(fl_pre)]\n",
    "\n",
    "# Inicializa una lista para almacenar los DataFrames cargados desde los archivos CSV\n",
    "dataframes = []\n",
    "\n",
    "# Carga cada archivo CSV en un DataFrame y agrega a la lista\n",
    "for archivo in archivos_csv:\n",
    "    ruta_completa = os.path.join(result_dir, archivo)\n",
    "    df = pd.read_csv(ruta_completa, sep=';', encoding='utf-16')\n",
    "    dataframes.append(df)\n",
    "    \n",
    "\n",
    "# Combina los DataFrames en uno solo\n",
    "portadas = pd.concat(dataframes, ignore_index=True)\n",
    "exportable = rd(portadas)\n",
    "print(len(exportable))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export the master file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sd(exportable, f'{monitoring_dir}/{fl_master}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
